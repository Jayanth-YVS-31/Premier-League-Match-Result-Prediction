{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6bfd871-93e9-4146-bb46-97b54a7190a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cd91fcb-43bd-426a-bea0-8fbf5f0c1aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "standings_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bec03d7c-4415-4ff4-953b-dc016c5766e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = requests.get(standings_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14317aec-652f-4a9b-9601-87e1fa33c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ff6d3f7-2fc0-40c5-98ee-687760f52810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 tables\n",
      "Table 0 classes: ['stats_table', 'sortable', 'min_width', 'force_mobilize']\n",
      "Table 1 classes: ['stats_table', 'sortable', 'min_width', 'force_mobilize']\n",
      "Table 2 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 3 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 4 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 5 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 6 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 7 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 8 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 9 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 10 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 11 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 12 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 13 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 14 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 15 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 16 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 17 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 18 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 19 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 20 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 21 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 22 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Table 23 classes: ['stats_table', 'sortable', 'min_width']\n",
      "Standings table found successfully.\n",
      "Found 20 team links\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "standings_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\"\n",
    "data = requests.get(standings_url, headers=HEADERS)\n",
    "\n",
    "# Parse the HTML response\n",
    "soup = BeautifulSoup(data.text, 'html.parser')\n",
    "\n",
    "# Debug: Print out available tables\n",
    "tables = soup.find_all('table')\n",
    "print(f\"Found {len(tables)} tables\")\n",
    "for i, table in enumerate(tables):\n",
    "    print(f\"Table {i} classes: {table.get('class')}\")\n",
    "\n",
    "# Try to select the stats table with error handling\n",
    "try:\n",
    "    standings_table = soup.select('table.stats_table')[0]\n",
    "    print(\"Standings table found successfully.\")\n",
    "except IndexError:\n",
    "    print(\"No stats_table found. The HTML structure might have changed.\")\n",
    "    exit()  # Exit the program or handle it in another way\n",
    "\n",
    "# Extract links from the table\n",
    "links = standings_table.find_all('a')\n",
    "links = [l.get(\"href\") for l in links if '/squads/' in l.get(\"href\")]\n",
    "print(f\"Found {len(links)} team links\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8f4cef8-4e0c-46ef-92be-8adfe5c9463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_urls = [f\"https://fbref.com{l}\" for l in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b904d6a6-aa6d-4d0f-b42b-71a1a8e17a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = requests.get(team_urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9432bd0-a243-451b-b113-d3e9aa23076c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://fbref.com/en/squads/822bd0ba/Liverpool-Stats',\n",
       " 'https://fbref.com/en/squads/b8fd03ef/Manchester-City-Stats',\n",
       " 'https://fbref.com/en/squads/18bb7c10/Arsenal-Stats',\n",
       " 'https://fbref.com/en/squads/8602292d/Aston-Villa-Stats',\n",
       " 'https://fbref.com/en/squads/d07537b9/Brighton-and-Hove-Albion-Stats',\n",
       " 'https://fbref.com/en/squads/cff3d9bb/Chelsea-Stats',\n",
       " 'https://fbref.com/en/squads/361ca564/Tottenham-Hotspur-Stats',\n",
       " 'https://fbref.com/en/squads/e4a775cb/Nottingham-Forest-Stats',\n",
       " 'https://fbref.com/en/squads/b2b47a98/Newcastle-United-Stats',\n",
       " 'https://fbref.com/en/squads/fd962109/Fulham-Stats',\n",
       " 'https://fbref.com/en/squads/4ba7cbea/Bournemouth-Stats',\n",
       " 'https://fbref.com/en/squads/19538871/Manchester-United-Stats',\n",
       " 'https://fbref.com/en/squads/cd051869/Brentford-Stats',\n",
       " 'https://fbref.com/en/squads/a2d435b3/Leicester-City-Stats',\n",
       " 'https://fbref.com/en/squads/7c21e445/West-Ham-United-Stats',\n",
       " 'https://fbref.com/en/squads/d3fd31cc/Everton-Stats',\n",
       " 'https://fbref.com/en/squads/b74092de/Ipswich-Town-Stats',\n",
       " 'https://fbref.com/en/squads/47c64c55/Crystal-Palace-Stats',\n",
       " 'https://fbref.com/en/squads/33c895d4/Southampton-Stats',\n",
       " 'https://fbref.com/en/squads/8cec06e1/Wolverhampton-Wanderers-Stats']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b94ebf7c-e72c-4832-9b04-7bf7999814bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91970\\AppData\\Local\\Temp\\ipykernel_33720\\4209044294.py:2: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No tables found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mScores & Fixtures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mC:\\PROJECTS\\sample_projects\\project1\\env\\lib\\site-packages\\pandas\\io\\html.py:1246\u001b[0m, in \u001b[0;36mread_html\u001b[1;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\u001b[0m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m   1231\u001b[0m     [\n\u001b[0;32m   1232\u001b[0m         is_file_like(io),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1236\u001b[0m     ]\n\u001b[0;32m   1237\u001b[0m ):\n\u001b[0;32m   1238\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal html to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_html\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1243\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1244\u001b[0m     )\n\u001b[1;32m-> 1246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\PROJECTS\\sample_projects\\project1\\env\\lib\\site-packages\\pandas\\io\\html.py:1009\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1008\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m retained \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retained\n\u001b[0;32m   1011\u001b[0m ret \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables:\n",
      "File \u001b[1;32mC:\\PROJECTS\\sample_projects\\project1\\env\\lib\\site-packages\\pandas\\io\\html.py:989\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    978\u001b[0m p \u001b[38;5;241m=\u001b[39m parser(\n\u001b[0;32m    979\u001b[0m     io,\n\u001b[0;32m    980\u001b[0m     compiled_match,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    985\u001b[0m     storage_options,\n\u001b[0;32m    986\u001b[0m )\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 989\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m caught:\n\u001b[0;32m    991\u001b[0m     \u001b[38;5;66;03m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[0;32m    992\u001b[0m     \u001b[38;5;66;03m# and try to rewind it before trying the next parser\u001b[39;00m\n\u001b[0;32m    993\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(io, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseekable\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m io\u001b[38;5;241m.\u001b[39mseekable():\n",
      "File \u001b[1;32mC:\\PROJECTS\\sample_projects\\project1\\env\\lib\\site-packages\\pandas\\io\\html.py:249\u001b[0m, in \u001b[0;36m_HtmlFrameParser.parse_tables\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_tables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    242\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m    Parse and return all tables from the DOM.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_thead_tbody_tfoot(table) \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables)\n",
      "File \u001b[1;32mC:\\PROJECTS\\sample_projects\\project1\\env\\lib\\site-packages\\pandas\\io\\html.py:604\u001b[0m, in \u001b[0;36m_BeautifulSoupHtml5LibFrameParser._parse_tables\u001b[1;34m(self, document, match, attrs)\u001b[0m\n\u001b[0;32m    602\u001b[0m tables \u001b[38;5;241m=\u001b[39m document\u001b[38;5;241m.\u001b[39mfind_all(element_name, attrs\u001b[38;5;241m=\u001b[39mattrs)\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tables:\n\u001b[1;32m--> 604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo tables found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    606\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    607\u001b[0m unique_tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: No tables found"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa927a12-0362-4a45-8d3c-803c32b8a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(data.text)\n",
    "links = soup.find_all('a')\n",
    "links = [l.get(\"href\") for l in links]\n",
    "links = [l for l in links if l and 'all_comps/shooting/' in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09af8cb-3533-47f3-b4f8-22f36514eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = requests.get(f\"https://fbref.com{links[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce95cd1-78e2-4411-a956-a74b43078262",
   "metadata": {},
   "outputs": [],
   "source": [
    "shooting = pd.read_html(data.text, match=\"Shooting\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a442054-0312-4e21-a662-0c8145f848d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shooting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b024f3a-a0b3-4b53-8dbd-b5649f1326ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "shooting.columns = shooting.columns.droplevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccac08ea-724c-4ea4-b4cb-615de33e167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_data = matches.merge(shooting[[\"Date\", \"Sh\", \"SoT\", \"Dist\", \"FK\", \"PK\", \"PKatt\"]], on=\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb16fa23-bf93-40fc-a479-f9ae4114f2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515b73f-3fc4-4514-b66b-4ba68dbfa1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2022, 2020, -1))\n",
    "all_matches = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0a4db9-54f0-48eb-a76f-5727f6404f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "standings_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff072127-6a65-4b3e-933d-e84b92bf120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# for year in years:\n",
    "#     data = requests.get(standings_url)\n",
    "#     soup = BeautifulSoup(data.text)\n",
    "#     standings_table = soup.select('table.stats_table')[0]\n",
    "\n",
    "#     links = [l.get(\"href\") for l in standings_table.find_all('a')]\n",
    "#     links = [l for l in links if '/squads/' in l]\n",
    "#     team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
    "    \n",
    "#     previous_season = soup.select(\"a.prev\")[0].get(\"href\")\n",
    "#     standings_url = f\"https://fbref.com{previous_season}\"\n",
    "    \n",
    "#     for team_url in team_urls:\n",
    "#         team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "#         data = requests.get(team_url)\n",
    "#         matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n",
    "#         soup = BeautifulSoup(data.text)\n",
    "#         links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "#         links = [l for l in links if l and 'all_comps/shooting/' in l]\n",
    "#         data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "#         shooting = pd.read_html(data.text, match=\"Shooting\")[0]\n",
    "#         shooting.columns = shooting.columns.droplevel()\n",
    "#         try:\n",
    "#             team_data = matches.merge(shooting[[\"Date\", \"Sh\", \"SoT\", \"Dist\", \"FK\", \"PK\", \"PKatt\"]], on=\"Date\")\n",
    "#         except ValueError:\n",
    "#             continue\n",
    "#         team_data = team_data[team_data[\"Comp\"] == \"Premier League\"]\n",
    "        \n",
    "#         team_data[\"Season\"] = year\n",
    "#         team_data[\"Team\"] = team_name\n",
    "#         all_matches.append(team_data)\n",
    "#         time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "433a834c-da4b-41c8-8b9a-d4d053bab7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching standings for year 2023: 429 Client Error: Too Many Requests for url: https://fbref.com/en/comps/9/Premier-League-Stats\n",
      "Fetching data for Liverpool - 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91970\\AppData\\Local\\Temp\\ipykernel_33720\\1691163807.py:50: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No shooting data found for Liverpool\n",
      "Fetching data for Manchester City - 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91970\\AppData\\Local\\Temp\\ipykernel_33720\\1691163807.py:50: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No shooting data found for Manchester City\n",
      "Fetching data for Arsenal - 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91970\\AppData\\Local\\Temp\\ipykernel_33720\\1691163807.py:50: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No shooting data found for Arsenal\n",
      "Fetching data for Aston Villa - 2022\n",
      "Error fetching matches for Aston Villa: 429 Client Error: Too Many Requests for url: https://fbref.com/en/squads/8602292d/Aston-Villa-Stats\n",
      "Fetching data for Brighton and Hove Albion - 2022\n",
      "Error fetching matches for Brighton and Hove Albion: 429 Client Error: Too Many Requests for url: https://fbref.com/en/squads/d07537b9/Brighton-and-Hove-Albion-Stats\n",
      "Fetching data for Chelsea - 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91970\\AppData\\Local\\Temp\\ipykernel_33720\\1691163807.py:50: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No shooting data found for Chelsea\n",
      "Fetching data for Tottenham Hotspur - 2022\n",
      "Error fetching matches for Tottenham Hotspur: 429 Client Error: Too Many Requests for url: https://fbref.com/en/squads/361ca564/Tottenham-Hotspur-Stats\n",
      "Fetching data for Nottingham Forest - 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91970\\AppData\\Local\\Temp\\ipykernel_33720\\1691163807.py:50: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No shooting data found for Nottingham Forest\n",
      "Fetching data for Newcastle United - 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91970\\AppData\\Local\\Temp\\ipykernel_33720\\1691163807.py:50: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No shooting data found for Newcastle United\n",
      "Fetching data for Fulham - 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91970\\AppData\\Local\\Temp\\ipykernel_33720\\1691163807.py:50: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No shooting data found for Fulham\n",
      "Fetching data for Bournemouth - 2022\n",
      "Error fetching matches for Bournemouth: 429 Client Error: Too Many Requests for url: https://fbref.com/en/squads/4ba7cbea/Bournemouth-Stats\n",
      "Fetching data for Manchester United - 2022\n",
      "Error fetching matches for Manchester United: 429 Client Error: Too Many Requests for url: https://fbref.com/en/squads/19538871/Manchester-United-Stats\n",
      "Fetching data for Brentford - 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91970\\AppData\\Local\\Temp\\ipykernel_33720\\1691163807.py:50: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No shooting data found for Brentford\n",
      "Fetching data for Leicester City - 2022\n",
      "Error fetching matches for Leicester City: 429 Client Error: Too Many Requests for url: https://fbref.com/en/squads/a2d435b3/Leicester-City-Stats\n",
      "Fetching data for West Ham United - 2022\n",
      "Error fetching matches for West Ham United: 429 Client Error: Too Many Requests for url: https://fbref.com/en/squads/7c21e445/West-Ham-United-Stats\n",
      "Fetching data for Everton - 2022\n",
      "Error fetching matches for Everton: 429 Client Error: Too Many Requests for url: https://fbref.com/en/squads/d3fd31cc/Everton-Stats\n",
      "Fetching data for Ipswich Town - 2022\n",
      "Error fetching matches for Ipswich Town: 429 Client Error: Too Many Requests for url: https://fbref.com/en/squads/b74092de/Ipswich-Town-Stats\n",
      "Fetching data for Crystal Palace - 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91970\\AppData\\Local\\Temp\\ipykernel_33720\\1691163807.py:50: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No shooting data found for Crystal Palace\n",
      "Fetching data for Southampton - 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91970\\AppData\\Local\\Temp\\ipykernel_33720\\1691163807.py:50: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No shooting data found for Southampton\n",
      "Fetching data for Wolverhampton Wanderers - 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91970\\AppData\\Local\\Temp\\ipykernel_33720\\1691163807.py:50: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No shooting data found for Wolverhampton Wanderers\n",
      "Error fetching standings for year 2021: 429 Client Error: Too Many Requests for url: https://fbref.com/en/comps/9/2023-2024/2023-2024-Premier-League-Stats\n",
      "No data scraped.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Headers to avoid getting blocked\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Initial URL for the Premier League standings\n",
    "standings_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\"\n",
    "all_matches = []\n",
    "\n",
    "years = [2023, 2022, 2021]  # Example years\n",
    "\n",
    "for year in years:\n",
    "    try:\n",
    "        # Fetch standings page\n",
    "        data = requests.get(standings_url, headers=HEADERS)\n",
    "        data.raise_for_status()  # Check for HTTP errors\n",
    "        soup = BeautifulSoup(data.text, 'html.parser')\n",
    "\n",
    "        # Extract the standings table\n",
    "        standings_table = soup.select('table.stats_table')[0]\n",
    "    except (IndexError, requests.RequestException) as e:\n",
    "        print(f\"Error fetching standings for year {year}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Extract team URLs\n",
    "    links = [l.get(\"href\") for l in standings_table.find_all('a') if '/squads/' in l.get(\"href\")]\n",
    "    team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
    "\n",
    "    # Get the previous season's link\n",
    "    try:\n",
    "        previous_season = soup.select(\"a.prev\")[0].get(\"href\")\n",
    "        standings_url = f\"https://fbref.com{previous_season}\"\n",
    "    except IndexError:\n",
    "        print(f\"No previous season link found for year {year}\")\n",
    "        break  # Stop if no previous season is found\n",
    "\n",
    "    for team_url in team_urls:\n",
    "        team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "        print(f\"Fetching data for {team_name} - {year}\")\n",
    "\n",
    "        try:\n",
    "            # Fetch team page and matches table\n",
    "            data = requests.get(team_url, headers=HEADERS)\n",
    "            data.raise_for_status()\n",
    "            matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n",
    "        except (ValueError, requests.RequestException) as e:\n",
    "            print(f\"Error fetching matches for {team_name}: {e}\")\n",
    "            continue\n",
    "        # Extract shooting stats link safely\n",
    "        links = [\n",
    "            l.get(\"href\") for l in soup.find_all('a') \n",
    "            if l.get(\"href\") and 'all_comps/shooting/' in l.get(\"href\")\n",
    "        ]\n",
    "        \n",
    "        if not links:\n",
    "            print(f\"No shooting data found for {team_name}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Fetch shooting stats page and data\n",
    "            shooting_url = f\"https://fbref.com{links[0]}\"\n",
    "            data = requests.get(shooting_url, headers=HEADERS)\n",
    "            data.raise_for_status()\n",
    "            shooting = pd.read_html(data.text, match=\"Shooting\")[0]\n",
    "            shooting.columns = shooting.columns.droplevel()  # Flatten multi-level headers\n",
    "        except (ValueError, requests.RequestException, IndexError) as e:\n",
    "            print(f\"Error fetching shooting data for {team_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        # Merge matches and shooting data on the 'Date' column\n",
    "        try:\n",
    "            team_data = matches.merge(\n",
    "                shooting[[\"Date\", \"Sh\", \"SoT\", \"Dist\", \"FK\", \"PK\", \"PKatt\"]],\n",
    "                on=\"Date\"\n",
    "            )\n",
    "        except ValueError as e:\n",
    "            print(f\"Error merging data for {team_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Filter only Premier League matches\n",
    "        team_data = team_data[team_data[\"Comp\"] == \"Premier League\"]\n",
    "\n",
    "        # Add season and team name columns\n",
    "        team_data[\"Season\"] = year\n",
    "        team_data[\"Team\"] = team_name\n",
    "\n",
    "        # Append to the main list\n",
    "        all_matches.append(team_data)\n",
    "\n",
    "        # Sleep to avoid getting blocked\n",
    "        time.sleep(3)\n",
    "\n",
    "# Combine all data into a single DataFrame\n",
    "if all_matches:\n",
    "    final_data = pd.concat(all_matches, ignore_index=True)\n",
    "    print(\"Data scraping completed successfully!\")\n",
    "else:\n",
    "    print(\"No data scraped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c2086-7aeb-45f4-a3c5-4f723eda98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e43f577-f96a-49d3-8c28-3933617ef207",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df = pd.concat(all_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c50681-09db-4ff0-b7c6-9b36dd648442",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df.columns = [c.lower() for c in match_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0915fcdf-6cdf-4acf-8ed4-5a847d95e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6c104-708f-4a9f-80fe-3e3085263a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df.to_csv(\"matches.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
